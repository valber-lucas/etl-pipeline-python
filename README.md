<img src="VLCODE.png" alt="logo" width="150">

#  High-Performance ETL Pipeline (Python)

Pipeline de Engenharia de Dados capaz de processar e ingerir datasets massivos com consumo de mem√≥ria constante.

## ‚ö° Performance Benchmark
- **Dataset:** 1 Milh√£o de registros.
- **Tempo de Processamento:** ~2.4 segundos.
- **Estrat√©gia:** Generators (Streaming) + Batch Insert (SQL).

## üõ†Ô∏è Tecnologias
- **Python 3.12+**
- **SQLite**
- **Logging**
- **CI/CD:** GitHub Actions

## ‚öôÔ∏è Arquitetura
O projeto resolve o problema de "Memory Overflow" ao ler arquivos maiores que a RAM dispon√≠vel:

1.  **Extract:** Leitura via `yield`.
2.  **Transform:** Normaliza√ß√£o e valida√ß√£o de tipos com `Type Hints`.
3.  **Load:** Inser√ß√£o em lotes (Batch Size: 5000) para otimizar I/O.

### Fluxo de Dados
```mermaid
graph LR
    A[CSV File] -->|Stream| B(Generator Python)
    B -->|Yield| C{Transform}
    C -->|Invalid| D[Log Error]
    C -->|Valid| E[Batch Buffer]
    E -->|Full?| F[(SQLite DB)]
    
    style A fill:#f9f,stroke:#333
    style F fill:#bbf,stroke:#333
```
## Como Rodar

**Gere o dataset de teste:**
- Bash:
python generate_data.py

**Execute o pipeline:**
- Bash:
python etl_processor.py

**Rode os testes unit√°rios:**
- Bash:
python test_etl.py
