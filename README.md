#  High-Performance ETL Pipeline (Python)

Pipeline de Engenharia de Dados capaz de processar e ingerir datasets massivos com consumo de mem√≥ria constante.

## ‚ö° Performance Benchmark
- **Dataset:** 1 Milh√£o de registros (CSV gerado sinteticamente).
- **Tempo de Processamento:** ~2.4 segundos.
- **Estrat√©gia:** Generators (Streaming) + Batch Insert (SQL).

## üõ†Ô∏è Tecnologias
- **Python 3.12+**
- **SQLite** (Persist√™ncia Relacional)
- **Logging** (Rastreabilidade Enterprise)
- **CI/CD:** GitHub Actions (Automated Testing)

## ‚öôÔ∏è Arquitetura
O projeto resolve o problema de "Memory Overflow" ao ler arquivos maiores que a RAM dispon√≠vel:

1.  **Extract:** Leitura via `yield` (Lazy Loading).
2.  **Transform:** Normaliza√ß√£o e valida√ß√£o de tipos com `Type Hints`.
3.  **Load:** Inser√ß√£o em lotes (Batch Size: 5000) para otimizar I/O.

### Fluxo de Dados
```mermaid
graph LR
    A[CSV File] -->|Stream| B(Generator Python)
    B -->|Yield| C{Transform}
    C -->|Invalid| D[Log Error]
    C -->|Valid| E[Batch Buffer]
    E -->|Full?| F[(SQLite DB)]
    
    style A fill:#f9f,stroke:#333
    style F fill:#bbf,stroke:#333
```
## Como Rodar

**Gere o dataset de teste:**
- Bash:
python generate_data.py

**Execute o pipeline:**
- Bash:
python etl_processor.py

**Rode os testes unit√°rios:**
- Bash:
python test_etl.py
