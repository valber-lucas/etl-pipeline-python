# High-Performance ETL Pipeline (Python)

Pipeline de Engenharia de Dados capaz de processar e ingerir datasets massivos com consumo de mem√≥ria constante (O(1)).

## ‚ö° Performance Benchmark
- **Dataset:** 1 Milh√£o de registros (CSV gerado sinteticamente).
- **Tempo de Processamento:** ~2.4 segundos.
- **Estrat√©gia:** Generators (Streaming) + Batch Insert (SQL).

## üõ†Ô∏è Tecnologias
- **Python 3.12+**
- **SQLite** (Persist√™ncia Relacional)
- **Logging** (Rastreabilidade Enterprise)
- **CI/CD:** GitHub Actions (Automated Testing)

## ‚öôÔ∏è Arquitetura
O projeto resolve o problema de "Memory Overflow" ao ler arquivos maiores que a RAM dispon√≠vel:

1.  **Extract:** Leitura via `yield` (Lazy Loading).
2.  **Transform:** Normaliza√ß√£o e valida√ß√£o de tipos com `Type Hints`.
3.  **Load:** Inser√ß√£o em lotes (Batch Size: 5000) para otimizar I/O.

## Como Rodar o Projeto

Gere o dataset de teste:

Bash
python generate_data.py

Execute o pipeline:

Bash
python etl_processor.py

Rode os testes unit√°rios:

Bash
python test_etl.py

### Fluxo de Dados
```mermaid
graph LR
    A[üìÑ CSV File<br>1M+ Rows] -->|Stream Lazy Loading| B(‚öôÔ∏è Generator Python)
    B -->|Yield Row| C{üîç Transform & Validate}
    C -->|Invalid| D[üóëÔ∏è Discard/Log]
    C -->|Valid| E[üì¶ Batch Buffer]
    E -->|Batch Full?| F[(üóÑÔ∏è SQLite Database)]
    
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style F fill:#bbf,stroke:#333,stroke-width:2px

## Como Rodar o Projeto
Gere o dataset de teste:

Bash

python generate_data.py
Execute o pipeline:

Bash

python etl_processor.py
Rode os testes unit√°rios:

Bash

python test_etl.py